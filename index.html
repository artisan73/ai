<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Eddy AI</title>
    <style>
        /* Updated holographic UI */
        .hologram-ui {
            position: fixed;
            width: 100vw;
            height: 100vh;
            background: radial-gradient(circle, rgba(0,15,40,0.9) 0%, rgba(0,0,0,0.9) 100%);
            border: 2px solid #00f7ff33;
            box-shadow: 0 0 50px rgba(0, 247, 255, 0.2);
        }

        .face-overlay {
            position: absolute;
            width: 300px;
            height: 300px;
            bottom: 50px;
            right: 50px;
        }

        .dynamic-eyes {
            position: absolute;
            width: 100%;
            height: 100%;
            filter: drop-shadow(0 0 15px #00f7ff);
        }

        .eye {
            position: absolute;
            width: 30px;
            height: 50px;
            background: #00f7ff;
            border-radius: 15px;
            transition: all 0.2s;
        }

        .eyebrow {
            position: absolute;
            width: 40px;
            height: 8px;
            background: #00f7ff;
            border-radius: 4px;
            transition: all 0.3s;
        }

        .emotion-indicator {
            position: absolute;
            top: 20px;
            left: 20px;
            color: #00f7ff;
            font-family: 'Courier New', monospace;
            text-shadow: 0 0 10px #00f7ff;
        }
    </style>
</head>
<body>
    <div class="hologram-ui">
        <video id="cameraFeed" autoplay style="display: none;"></video>
        <canvas id="faceCanvas"></canvas>
        <div class="face-overlay">
            <div class="eyebrow" id="leftBrow" style="left: 25%; top: 25%;"></div>
            <div class="eyebrow" id="rightBrow" style="right: 25%; top: 25%;"></div>
            <div class="eye" id="leftEye" style="left: 25%; top: 30%;"></div>
            <div class="eye" id="rightEye" style="right: 25%; top: 30%;"></div>
            <div class="mouth" id="mouth" style="bottom: 25%; left: 50%;"></div>
        </div>
        <div class="emotion-indicator" id="emotion">Analyzing...</div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-expression-recognition"></script>

    <script>
        const emotionElement = document.getElementById('emotion');
        const video = document.getElementById('cameraFeed');
        const canvas = document.getElementById('faceCanvas');
        const ctx = canvas.getContext('2d');
        
        // UI Elements
        const mouth = document.getElementById('mouth');
        const leftEye = document.getElementById('leftEye');
        const rightEye = document.getElementById('rightEye');
        const leftBrow = document.getElementById('leftBrow');
        const rightBrow = document.getElementById('rightBrow');

        let currentEmotion = 'neutral';
        let isAnalyzing = true;
        let modelsLoaded = false;

        async function setupCamera() {
            canvas.width = window.innerWidth;
            canvas.height = window.innerHeight;
            
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: { facingMode: 'environment' },
                    audio: false
                });
                video.srcObject = stream;
                video.play();
                return new Promise((resolve) => {
                    video.onloadedmetadata = () => resolve();
                });
            } catch (error) {
                console.error('Camera error:', error);
            }
        }

        async function loadModels() {
            const [faceModel, emotionModel] = await Promise.all([
                faceLandmarksDetection.load(
                    faceLandmarksDetection.SupportedPackages.mediapipeFacemesh
                ),
                tf.loadLayersModel('https://tfhub.dev/google/tfjs-model/tfjs_audioset_emotion/1')
            ]);
            modelsLoaded = true;
            return { faceModel, emotionModel };
        }

        async function analyzeFace() {
            if (!modelsLoaded) return;
            
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            const faces = await faceLandmarksDetection.detectFaces(canvas);
            
            if (faces.length > 0) {
                const face = faces[0];
                updateFaceElements(face);
                detectEmotion(face);
            }
            requestAnimationFrame(analyzeFace);
        }

        function updateFaceElements(face) {
            // Eye tracking
            const leftEyePos = face.annotations.leftEyeUpper0[3];
            const rightEyePos = face.annotations.rightEyeUpper0[3];
            
            leftEye.style.transform = `translate(${leftEyePos.x}px, ${leftEyePos.y}px)`;
            rightEye.style.transform = `translate(${rightEyePos.x}px, ${rightEyePos.y}px)`;

            // Eyebrow movement
            const browHeight = face.annotations.leftEyebrowUpper[3].y;
            leftBrow.style.top = `${browHeight - 20}px`;
            rightBrow.style.top = `${browHeight - 20}px`;

            // Mouth dynamics
            const mouthOpenness = face.annotations.lipsUpperInner[4].y - 
                                face.annotations.lipsLowerInner[4].y;
            mouth.style.height = `${Math.min(40, mouthOpenness)}px`;
        }

        async function detectEmotion(face) {
            const tensor = tf.browser.fromPixels(canvas)
                .resizeNearestNeighbor([48, 48])
                .toFloat()
                .expandDims();
            
            const prediction = await emotionModel.predict(tensor);
            const emotions = await prediction.data();
            currentEmotion = ['angry', 'disgust', 'fear', 'happy', 
                             'neutral', 'sad', 'surprise'][emotions.indexOf(Math.max(...emotions))];
            
            emotionElement.textContent = `Current mood: ${currentEmotion}`;
            updateVoiceResponse();
        }

        function updateVoiceResponse() {
            if (Math.random() < 0.1) { // 10% chance to comment
                const responses = {
                    happy: "You seem pleased with current conditions",
                    surprised: "I detect elevated surprise levels",
                    angry: "Recommend stress reduction protocols",
                    neutral: "Environment status: nominal"
                };
                speak(responses[currentEmotion] || "Systems operational");
            }
        }

        async function init() {
            await setupCamera();
            await loadModels();
            analyzeFace();
            setupVoiceControl();
            speak("Enhanced vision systems activated");
        }

        init();
    </script>
</body>
</html>
